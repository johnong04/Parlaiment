# Developer's Log: Malaysian Hansard Analytics

## Phase 1: Data Ingestion (Complete)

**Status:** 16,050 speeches extracted and enriched with Party/Coalition metadata.

### 1. Technical Decisions & Logic

- **State-Machine Parsing:** Switched from page-based to line-based parsing. This handles speeches spanning multiple pages by "staying open" until the next speaker is detected.
- **Regex Refinement:** Used `^([A-Z][\w\s\'\.\-@]{2,60})(?:\s*\[(.*?)\])?\s*:` to safely capture names and constituencies while avoiding accidental sentence matching.
- **Fuzzy Metadata Merge:** Implemented a two-tier match. Primary key: **Constituency** (unique/stable). Fallback: **Core Name** (stripping titles like _Dato, Dr., Tan Sri_) to handle inconsistent naming in PDFs.
- **Neutral Handling:** Tagged parliamentary moderators as `OFFICIAL` to ensure political sentiment analysis remains unbiased.

### 2. Key Data Insights (For Presentation)

- **Volume:** Extracted **16,050 speech turns** from 29 PDFs.
- **The "Marathon" Session:** `DR-19112018.pdf` was the most active file, containing a staggering **1,111 speeches** in a single day.
- **Speaker Diversity:** Identified **298 unique speakers**, covering almost the entire Dewan Rakyat membership.
- **Data Quality:** Achieved a **93.1% match rate** for political parties. The remaining 6.9% are mostly anonymous interjections ("Seorang Ahli"), which is expected in Hansard records.

### 3. Implementation Hurdles

- **PDF Noise:** TOC entries (e.g., "RANG UNDANG-UNDANG") triggered false speaker detections. Resolved by implementing a custom `SPEAKER_BLACKLIST`.
- **Resource Intensity:** Large PDFs (100+ pages) required significant processing time (~1hr for full batch).
- **Cross-Platform:** Adopted a "User-AI Delegation" model for environment setup (venv/pip) to avoid Windows-specific terminal errors.

---

## Phase 2: Modeling (In Progress)

### 1. Topic Modeling (BERTopic) - Implementation Details

- **Script:** Created `src/train_topics.py` designed for Google Colab/Local execution.
- **Model Selection:** Switched to `paraphrase-multilingual-MiniLM-L12-v2` for the embedding layer to properly handle Malay/English Hansard text.
- **Refinement:** Used `KeyBERTInspired` representation model to generate cleaner, more descriptive topic labels compared to raw c-TF-IDF.
- **Workflow:** Implemented a two-stage training process (1,000-row test run followed by full 16,000-row training).
- **Visualization:** Integrated `topics_over_time` to generate `data/topic_chart.html`.

### 2. Topic Modeling - Findings & Presentation Content
- **Coherence Score (Cv):** **0.3909** (Evaluated on 48 topics). This provides quantitative validation that the clusters are semantically consistent.
- **Key Discovery:** Topic 0 (Political Banter) correctly isolated heckling and interjections (Keywords: *Tajuddin, Shahidan, Rayer*).
- **Thematic Topics:** Topic 5 (Education) and Topic 10 (Budget/Finance) show clear temporal spikes corresponding to parliamentary cycles.
- **Data Quality:** Outlier topic (-1) successfully captured ~37% of "procedural noise," keeping the thematic topics clean.

### 3. Stance Modeling (XLM-RoBERTa) - Implementation Details
- **Model:** `xlm-roberta-base` fine-tuned on 600 "Silver Labels" generated by Manus AI.
- **Accuracy:** Achieved **69.1%** overall accuracy and **0.68 Macro F1-Score**.
- **Optimization:** Implemented `WeightedTrainer` to handle class imbalance, specifically boosting "Pro" and "Evasive" performance.
- **Inference:** Processed all 16,050 rows in under 15 minutes using batch inference.

### 4. Key Insights for Presentation (Phase 2 Findings)
- **The "Banter" Baseline:** 37% of parliamentary speech was identified as procedural noise or political banter, successfully isolated by the model.
- **Evasiveness Trends:** Early analysis shows that Evasiveness counts are significantly higher during Budget Debates (Topic 10) compared to general procedural sessions.
- **Coalition Dynamics:** Preliminary data indicates a distinct stance profile difference between Government (higher "Pro" and "Evasive") and Opposition (higher "Con") coalitions.
- **Scientific Validation:** Confusion Matrix confirms that the model distinguishes between "Neutral" and "Evasive" with 70% precision, minimizing false accusations of evasiveness.
